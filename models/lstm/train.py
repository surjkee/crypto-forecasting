# models/lstm/train.py

from pathlib import Path
from typing import Dict, Any

import numpy as np
import torch
from torch import nn
from torch.utils.data import DataLoader

from config.settings import get_settings
from data.db import load_ohlcv_hourly
from models.lstm.config import LSTMConfig
from models.lstm.dataset import prepare_datasets_and_scaler
from models.lstm.model import LSTMForecastModel



def _to_device():
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")


def _train_one_epoch(
    model: nn.Module,
    loader: DataLoader,
    criterion: nn.Module,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
) -> float:
    model.train()
    total_loss = 0.0
    n_batches = 0

    for x_batch, y_batch in loader:
        x_batch = x_batch.to(device)
        y_batch = y_batch.to(device)

        optimizer.zero_grad()
        y_pred = model(x_batch).squeeze(-1)   # (batch,)
        y_true = y_batch.squeeze(-1)          # (batch,)

        loss = criterion(y_pred, y_true)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        n_batches += 1

    return total_loss / max(n_batches, 1)


def _evaluate(
    model: nn.Module,
    loader: DataLoader,
    criterion: nn.Module,
    device: torch.device,
) -> float:
    model.eval()
    total_loss = 0.0
    n_batches = 0

    with torch.no_grad():
        for x_batch, y_batch in loader:
            x_batch = x_batch.to(device)
            y_batch = y_batch.to(device)

            y_pred = model(x_batch).squeeze(-1)
            y_true = y_batch.squeeze(-1)

            loss = criterion(y_pred, y_true)

            total_loss += loss.item()
            n_batches += 1

    return total_loss / max(n_batches, 1)


def _inverse_scale_target(
    scaler,
    feature_cols,
    target_col_idx: int,
    y_scaled: np.ndarray,
) -> np.ndarray:
    """
    –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ y_scaled (N,) –Ω–∞–∑–∞–¥ –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –º–∞—Å—à—Ç–∞–± —á–µ—Ä–µ–∑ scaler.inverse_transform.

    –†–æ–±–∏–º–æ —Ç—Ä—é–∫:
    - —Å—Ç–≤–æ—Ä—é—î–º–æ –Ω—É–ª—å–æ–≤—É –º–∞—Ç—Ä–∏—Ü—é (N, num_features)
    - –≤ –∫–æ–ª–æ–Ω–∫—É target_col_idx –∫–ª–∞–¥–µ–º–æ y_scaled
    - –≥–∞–Ω—è—î–º–æ —á–µ—Ä–µ–∑ inverse_transform
    - –±–µ—Ä–µ–º–æ —Ç—É –∂ —Å–∞–º—É –∫–æ–ª–æ–Ω–∫—É
    """
    num_features = len(feature_cols)
    tmp = np.zeros((len(y_scaled), num_features), dtype=np.float32)
    tmp[:, target_col_idx] = y_scaled

    inv = scaler.inverse_transform(tmp)
    y_inv = inv[:, target_col_idx]
    return y_inv


def train_lstm_for_coin(
    coin_id: str,
    vs_currency: str = None,
    config: LSTMConfig | None = None,
) -> Dict[str, Any]:
    """
    –ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª:
    - –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î –¥–∞–Ω—ñ –∑ DuckDB
    - –±—É–¥—É—î —Ñ—ñ—á—ñ
    - –≥–æ—Ç—É—î –¥–∞—Ç–∞—Å–µ—Ç–∏
    - —Ç—Ä–µ–Ω—É—î LSTM
    - —Ä–∞—Ö—É—î —Ç–µ—Å—Ç–æ–≤—ñ MAE/RMSE –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±—ñ —Ü—ñ–Ω–∏
    - –∑–±–µ—Ä—ñ–≥–∞—î —á–µ–∫–ø–æ—ñ–Ω—Ç
    """
    settings = get_settings()
    vs_currency = vs_currency or settings.default_vs_currency
    cfg = config or LSTMConfig()

    print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ –¥–ª—è {coin_id} ({vs_currency})...")
    df_raw = load_ohlcv_hourly(coin_id, vs_currency)

    if df_raw.empty:
        raise RuntimeError(
            f"–£ DuckDB –Ω–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –º–æ–Ω–µ—Ç–∏ '{coin_id}' ({vs_currency}). "
            f"–°–ø–æ—á–∞—Ç–∫—É –∑–∞–ø—É—Å—Ç–∏ jobs.fetch_history."
        )

    print("üßÆ –ì–æ—Ç—É—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç–∏ —Ç–∞ —Å–∫–µ–π–ª–µ—Ä...")
    (
        train_loader,
        test_loader,
        scaler,
        feature_cols,
        target_col_idx,
        train_scaled,
        test_scaled,
    ) = prepare_datasets_and_scaler(df_raw, cfg)

    input_size = len(feature_cols)
    device = _to_device()
    print(f"üîß –ü—Ä–∏—Å—Ç—Ä—ñ–π: {device}, input_size={input_size}")

    model = LSTMForecastModel(
        input_size=input_size,
        hidden_size=cfg.hidden_size,
        num_layers=cfg.num_layers,
        dropout=cfg.dropout,
    ).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)

    # --- –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è ---
    print("üöÇ –ü–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è LSTM...")
    for epoch in range(1, cfg.num_epochs + 1):
        train_loss = _train_one_epoch(model, train_loader, criterion, optimizer, device)
        val_loss = _evaluate(model, test_loader, criterion, device)

        print(
            f"[Epoch {epoch:03d}/{cfg.num_epochs}] "
            f"train_loss={train_loss:.6f}, val_loss={val_loss:.6f}"
        )

    # --- –û—Ü—ñ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç—ñ –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É –º–∞—Å—à—Ç–∞–±—ñ ---
    print("üìè –û—Ü—ñ–Ω—é—î–º–æ –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç—ñ...")
    model.eval()

    y_true_scaled_list = []
    y_pred_scaled_list = []

    with torch.no_grad():
        for x_batch, y_batch in test_loader:
            x_batch = x_batch.to(device)
            y_batch = y_batch.to(device)

            y_pred = model(x_batch).squeeze(-1)
            y_true = y_batch.squeeze(-1)

            y_true_scaled_list.append(y_true.cpu().numpy())
            y_pred_scaled_list.append(y_pred.cpu().numpy())

    y_true_scaled = np.concatenate(y_true_scaled_list, axis=0)
    y_pred_scaled = np.concatenate(y_pred_scaled_list, axis=0)

    y_true = _inverse_scale_target(scaler, feature_cols, target_col_idx, y_true_scaled)
    y_pred = _inverse_scale_target(scaler, feature_cols, target_col_idx, y_pred_scaled)

    mae = np.mean(np.abs(y_true - y_pred))
    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))

    print(f"‚úÖ Test MAE  = {mae:.4f} {vs_currency.upper()}")
    print(f"‚úÖ Test RMSE = {rmse:.4f} {vs_currency.upper()}")

    # --- –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —á–µ–∫–ø–æ—ñ–Ω—Ç ---
    artifact_path: Path = cfg.artifact_path(coin_id, vs_currency)
    checkpoint = {
        "config": cfg.__dict__,
        "feature_cols": feature_cols,
        "target_col_idx": target_col_idx,
        "state_dict": model.state_dict(),
        "scaler": scaler,
    }

    torch.save(checkpoint, artifact_path)
    print(f"üíæ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {artifact_path}")

    return {
        "artifact_path": artifact_path,
        "mae": float(mae),
        "rmse": float(rmse),
        "config": cfg,
    }
